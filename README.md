# Relativistic Gradient Descent (RGD)

RGD is an optimization method based on the simulation of a relativistic particle under the influence of a potential (objective function) and friction. We use a symplectic integrator to simulate the system.  RGD generalizes the classical momentum method (Polyak's heavy ball) and the accelerated gradient descent (Nesterov's method), and have usually superior performance.

This method was proposed in the G. Fran√ßa et. al., "Conformal symplectic and relativistic optimization,"  J. Stat. Mech. (2020) 124008 (https://iopscience.iop.org/article/10.1088/1742-5468/abcaee)

A shorter version of this paper was also published at NeurIPS 2020 (spotlight):
https://proceedings.neurips.cc/paper/2020/hash/c4b108f53550f1d5967305a9a8140ddd-Abstract.html
